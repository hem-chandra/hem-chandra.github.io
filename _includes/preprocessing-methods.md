## ğŸ‘‹ Hi, welcome!

We are increasingly using machine learning (ML) software to make autonomous decisionsâ€”such as identifying credit risks, determining criminal sentencing, discharging patients, predicting heart diseases, hiring employees, and more. 

However, biased decisions made by ML software against specific social groupsâ€”often based on protected or sensitive attributes (e.g., sex)â€”have raised significant concerns within both the software engineering (SE) and ML communities.

**ML software learns its decision logic from training data. As a result, if the training data is biased, the model can learn and perpetuate those biases in its predictions.**

---

### ğŸ› ï¸ This Page Covers: Pre-processing Methods for Fairness

This page highlights our **preprocessing-based fairness work**. These methods operate on the **training data** before any model is built. Techniques in this category include:

- Reweighting training examples  
- Data transformation or balancing  
- Biased label correction  
- Synthetic data generation to ensure equitable representation  

These approaches are essential for reducing or eliminating bias from the source, ensuring that downstream models behave more fairly and responsibly.

---

Feel free to explore the methods and publications listed here to learn more!










