## ðŸ‘‹ Hi, welcome!

This page highlights our preprocessing-based fairness methods, which operate on the training data before any machine learning (ML) model is built. These approaches aim to reduce or eliminate bias at the data source, as ML models learn statistical patterns directly from the training data. When bias exists in the data, it can lead to unfair or discriminatory outcomes during prediction. By addressing bias prior to model training, these methods help ensure that downstream models behave more fairly and responsibly.
 {% include_relative _includes/preprocessing-publications.md %}  
