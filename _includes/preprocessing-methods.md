## 👋 Hi, welcome!

### 🛠️ Pre-processing Methods for Fairness

This page highlights our **preprocessing-based fairness work**—methods that operate on the **training data** before any model is built. These approaches help reduce or eliminate bias at the source, ensuring downstream models behave more fairly and responsibly.

Machine learning (ML) software is increasingly used to make autonomous decisions in areas such as credit risk assessment, criminal sentencing, patient discharge, heart disease prediction, and hiring. However, biased decisions—often based on protected attributes like sex—have raised serious concerns in both the software engineering (SE) and ML communities.

**Since ML models learn from training data, any bias in that data can result in biased predictions.**
