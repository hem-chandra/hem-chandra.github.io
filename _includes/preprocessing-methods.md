## 👋 Hi, welcome!

We are increasingly using machine learning (ML) software to make autonomous decisions—such as identifying credit risks, determining criminal sentencing, discharging patients, predicting heart diseases, hiring employees, and more. 

However, biased decisions made by ML software against specific social groups—often based on protected or sensitive attributes (e.g., sex)—have raised significant concerns within both the software engineering (SE) and ML communities.

**ML software learns its decision logic from training data. As a result, if the training data is biased, the model can learn and perpetuate those biases in its predictions.**

---

### 🛠️ This Page Covers: Pre-processing Methods for Fairness

This page highlights our **preprocessing-based fairness work**. These methods operate on the **training data** before any model is built. Techniques in this category include:

- Reweighting training examples  
- Data transformation or balancing  
- Biased label correction  
- Synthetic data generation to ensure equitable representation  

These approaches are essential for reducing or eliminating bias from the source, ensuring that downstream models behave more fairly and responsibly.

---

Feel free to explore the methods and publications listed here to learn more!










